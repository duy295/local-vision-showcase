import torch
import torch.optim as optim
from torch.utils.data import DataLoader
import argparse
import os
import sys

# ƒê·∫£m b·∫£o python t√¨m th·∫•y c√°c file trong th∆∞ m·ª•c con
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# --- IMPORT MODULES ---
from backbone.feature_extract import HybridResNetBackbone
from backbone.relation_net import RelationNetwork
from backbone.loss import StructureAwareClipLoss
from utils.samplers import ClassSpecificBatchSampler
from utils.data_loader import CUB200_First10 
from torchvision import transforms

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--root', type=str, default='./CUB_200_2011', help='ƒê∆∞·ªùng d·∫´n folder dataset')
    parser.add_argument('--clip_path', type=str, default='./data/cub_clip.json')
    parser.add_argument('--epochs', type=int, default=5, help='Ch·∫°y th·ª≠ 5 epoch th√¥i')
    parser.add_argument('--batch_size', type=int, default=16)
    parser.add_argument('--lr', type=float, default=1e-4)
    return parser.parse_args()

def main():
    args = get_args()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üöÄ Running Test on: {device}")

    # 1. Setup Data & Model
    transform = transforms.Compose([
        transforms.Resize((256, 256)), 
        transforms.CenterCrop(224),
        transforms.ToTensor(), 
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    print("‚è≥ ƒêang chu·∫©n b·ªã d·ªØ li·ªáu 10 class...")
    # Class n√†y s·∫Ω t·ª± check, n·∫øu c√≥ folder CUB th·∫≠t th√¨ load 10 class, kh√¥ng th√¨ t·∫°o gi·∫£
    train_set = CUB200_First10(args.root, train=True, transform=transform)
    
    # L·∫•y label list cho Sampler
    # L∆∞u √Ω: Class loader ·ªü b∆∞·ªõc 2 ƒë√£ c√≥ property .data tr·∫£ v·ªÅ dict ch·ª©a label
    labels = train_set.data['label']

    backbone = HybridResNetBackbone().to(device)
    relation = RelationNetwork().to(device)
    loss_fn = StructureAwareClipLoss(args.clip_path, device=device) # T·ª± t·∫°o CLIP gi·∫£ b√™n trong n·∫øu thi·∫øu
    
    optimizer = optim.Adam(list(backbone.parameters()) + list(relation.parameters()), lr=args.lr)

    # 2. Chi·∫øn thu·∫≠t Train Test (Ng·∫Øn g·ªçn)
    # V√¨ test nhanh 10 class n√™n ta chia phase ng·∫Øn l·∫°i
    phase1_end = 2  # 2 epoch ƒë·∫ßu h·ªçc c·∫•u tr√∫c
    phase2_end = 4  # 2 epoch sau h·ªçc ph√¢n bi·ªát
    # Epoch 5: Shuffle
    
    print("\n>>> B·∫ÆT ƒê·∫¶U TEST LU·ªíNG (SANITY CHECK)")
    
    for epoch in range(args.epochs):
        backbone.train()
        relation.train()
        
        # --- CH·ªåN SAMPLER ---
        if epoch < phase1_end:
            print(f"[Phase 1] Epoch {epoch+1}: Structure Learning (Same Class Batch)")
            sampler = ClassSpecificBatchSampler(labels, args.batch_size)
            loader = DataLoader(train_set, batch_sampler=sampler)
        elif epoch < phase2_end:
            print(f"[Phase 2] Epoch {epoch+1}: Discrimination (Mixed Batch)")
            loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True)
        else:
            print(f"[Phase 3] Epoch {epoch+1}: Regularization (Shuffle)")
            loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True)

        total_loss = 0
        batch_count = 0
        
        for imgs, lbls, _ in loader:
            imgs, lbls = imgs.to(device), lbls.to(device)
            if imgs.size(0) < 2: continue
            
            # Split Batch
            curr_bs = imgs.size(0)
            if curr_bs % 2 != 0: 
                imgs = imgs[:-1]; lbls = lbls[:-1]; curr_bs -= 1
            
            half = curr_bs // 2
            img1, img2 = imgs[:half], imgs[half:]
            lbl1, lbl2 = lbls[:half], lbls[half:]
            
            # Forward
            feat1 = backbone(img1)
            feat2 = backbone(img2)
            scores = relation(feat1, feat2)
            
            loss = loss_fn(scores, feat1, feat2, lbl1, lbl2)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            batch_count += 1
            
            if batch_count % 5 == 0:
                 print(f"   Iter {batch_count}: Loss {loss.item():.4f}")
            
        print(f"   ==> Avg Loss Epoch {epoch+1}: {total_loss/max(batch_count, 1):.4f}\n")

    print("‚úÖ TEST TH√ÄNH C√îNG! Model ho·∫°t ƒë·ªông t·ªët.")
    # Kh√¥ng c·∫ßn l∆∞u model hay t√≠nh Ec v√¨ ƒë√¢y ch·ªâ l√† ch·∫°y th·ª≠

if __name__ == "__main__":
    main()