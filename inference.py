"""
HÃ m inference Ä‘á»ƒ so sÃ¡nh 2 áº£nh báº¥t ká»³ vÃ  láº¥y concept-based similarity score
"""

import torch
import torch.nn.functional as F
from torchvision import transforms
from PIL import Image
import json
import os
import sys

# Add paths
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from backbone.feature_extract import HybridResNetBackbone
from backbone.relation_net import BilinearRelationNet

class ImageSimilarityComparator:
    def __init__(self, backbone_path, relation_path, concept_embeddings_dir=None, device='cuda' if torch.cuda.is_available() else 'cpu'):
        """
        Khá»Ÿi táº¡o comparator Ä‘á»ƒ so sÃ¡nh 2 áº£nh
        
        Args:
            backbone_path: Path tá»›i checkpoint cá»§a HybridResNetBackbone
            relation_path: Path tá»›i checkpoint cá»§a BilinearRelationNet
            concept_embeddings_dir: Path tá»›i thÆ° má»¥c chá»©a CLIP embeddings (output_json/CUB200, v.v.)
            device: cuda hoáº·c cpu
        """
        self.device = device
        
        # Load models
        self.backbone = HybridResNetBackbone(output_dim=512).to(device)
        self.relation = BilinearRelationNet(input_dim=512, hidden_dim=256).to(device)
        
        # Load checkpoints
        if os.path.exists(backbone_path):
            self.backbone.load_state_dict(torch.load(backbone_path, map_location=device))
            print(f"âœ“ Loaded backbone from {backbone_path}")
        else:
            print(f"âš ï¸ Backbone checkpoint khÃ´ng tÃ¬m tháº¥y: {backbone_path}")
        
        if os.path.exists(relation_path):
            self.relation.load_state_dict(torch.load(relation_path, map_location=device))
            print(f"âœ“ Loaded relation net from {relation_path}")
        else:
            print(f"âš ï¸ Relation net checkpoint khÃ´ng tÃ¬m tháº¥y: {relation_path}")
        
        # Set to eval mode
        self.backbone.eval()
        self.relation.eval()
        
        # Load concept embeddings (náº¿u cÃ³)
        self.concept_embeddings = {}
        if concept_embeddings_dir and os.path.exists(concept_embeddings_dir):
            self._load_concept_embeddings(concept_embeddings_dir)
        
        # Image preprocessing
        self.transform = transforms.Compose([
            transforms.Resize(224),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], 
                                [0.229, 0.224, 0.225])
        ])
    
    def _load_concept_embeddings(self, concept_dir):
        """Load CLIP concept embeddings tá»« JSON files"""
        if not os.path.exists(concept_dir):
            return
        
        try:
            for filename in os.listdir(concept_dir):
                if filename.endswith('_final.json'):
                    concept_name = filename.replace('_final.json', '')
                    json_path = os.path.join(concept_dir, filename)
                    with open(json_path, 'r') as f:
                        emb = torch.tensor(json.load(f), dtype=torch.float32, device=self.device)
                        emb = F.normalize(emb, p=2, dim=0)
                        self.concept_embeddings[concept_name] = emb
            
            print(f"âœ“ Loaded {len(self.concept_embeddings)} concept embeddings")
        except Exception as e:
            print(f"âš ï¸ Lá»—i load concept embeddings: {e}")
    
    def load_image(self, image_path):
        """Load vÃ  preprocess áº£nh"""
        try:
            img = Image.open(image_path).convert('RGB')
            img_tensor = self.transform(img).unsqueeze(0).to(self.device)
            return img_tensor
        except Exception as e:
            print(f"âŒ Lá»—i load áº£nh {image_path}: {e}")
            return None
    
    def extract_features(self, image_path_or_tensor):
        """Extract feature tá»« 1 áº£nh"""
        with torch.no_grad():
            # Load áº£nh náº¿u lÃ  path
            if isinstance(image_path_or_tensor, str):
                img_tensor = self.load_image(image_path_or_tensor)
                if img_tensor is None:
                    return None
            else:
                img_tensor = image_path_or_tensor
            
            # Extract features
            features = self.backbone.forward_single(img_tensor)  # [1, 512]
            features = F.normalize(features, p=2, dim=1)  # Normalize
            
            return features.squeeze(0)  # [512]
    
    def compare_images(self, image1_path, image2_path, 
                      use_visual_only=False, 
                      verbose=True):
        """
        So sÃ¡nh 2 áº£nh vÃ  láº¥y similarity score
        
        Args:
            image1_path: Path áº£nh 1 (hoáº·c tensor)
            image2_path: Path áº£nh 2 (hoáº·c tensor)
            use_visual_only: Náº¿u True, chá»‰ dÃ¹ng visual similarity; 
                           Náº¿u False, káº¿t há»£p visual + concept
            verbose: In chi tiáº¿t scores
        
        Returns:
            dict vá»›i keys:
            - visual_score: Score tá»« BilinearRelationNet (0-1)
            - concept_score: Score tá»« concept embeddings (náº¿u cÃ³)
            - final_score: Score káº¿t há»£p (0-1)
        """
        # Extract features
        feat1 = self.extract_features(image1_path)
        feat2 = self.extract_features(image2_path)
        
        if feat1 is None or feat2 is None:
            return None
        
        # Visual similarity tá»« relation net
        with torch.no_grad():
            feat1_batch = feat1.unsqueeze(0)
            feat2_batch = feat2.unsqueeze(0)
            visual_score = self.relation(feat1_batch, feat2_batch).item()
            visual_score = max(0.0, min(1.0, visual_score))  # Clamp to [0, 1]
        
        # Concept similarity (náº¿u cÃ³ embeddings)
        concept_score = None
        if not use_visual_only and len(self.concept_embeddings) > 0:
            concept_score = self._compute_concept_similarity(feat1, feat2)
        
        # Final score
        if concept_score is not None:
            # Káº¿t há»£p: 70% visual + 30% concept
            final_score = 0.7 * visual_score + 0.3 * concept_score
        else:
            final_score = visual_score
        
        result = {
            'visual_score': visual_score,
            'concept_score': concept_score,
            'final_score': final_score
        }
        
        if verbose:
            self._print_comparison_result(result)
        
        return result
    
    def _compute_concept_similarity(self, feat1, feat2):
        """
        TÃ­nh concept similarity báº±ng cÃ¡ch so sÃ¡nh features vá»›i concept embeddings
        """
        if len(self.concept_embeddings) == 0:
            return None
        
        # TÃ­nh similarity vá»›i táº¥t cáº£ concept embeddings
        max_sim1 = 0.0
        max_sim2 = 0.0
        
        for concept_emb in self.concept_embeddings.values():
            sim1 = F.cosine_similarity(feat1.unsqueeze(0), concept_emb.unsqueeze(0)).item()
            sim2 = F.cosine_similarity(feat2.unsqueeze(0), concept_emb.unsqueeze(0)).item()
            
            max_sim1 = max(max_sim1, sim1)
            max_sim2 = max(max_sim2, sim2)
        
        # Concept score: náº¿u cáº£ 2 áº£nh gáº§n tá»›i cÃ¹ng 1 concept â†’ score cao
        concept_score = (max_sim1 + max_sim2) / 2.0  # Average proximity
        concept_score = (concept_score + 1.0) / 2.0  # Scale to [0, 1] tá»« [-1, 1]
        
        return concept_score
    
    def _print_comparison_result(self, result):
        """Pretty print comparison results"""
        print("\n" + "="*60)
        print("ðŸ“Š IMAGE COMPARISON RESULTS")
        print("="*60)
        print(f"Visual Score        : {result['visual_score']:.4f} (BilinearRelationNet)")
        if result['concept_score'] is not None:
            print(f"Concept Score       : {result['concept_score']:.4f} (CLIP Embeddings)")
        print(f"{'â”€'*60}")
        print(f"FINAL SCORE         : {result['final_score']:.4f}")
        print("="*60 + "\n")


def demo_comparison(image1_path, image2_path, 
                   backbone_path='./weights/backbone_full.pth',
                   relation_path='./weights/relation_full.pth',
                   concept_dir='./output_json/CUB200'):
    """
    Quick demo function
    """
    print("ðŸš€ Initializing comparator...")
    comparator = ImageSimilarityComparator(
        backbone_path=backbone_path,
        relation_path=relation_path,
        concept_embeddings_dir=concept_dir,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    print(f"\nðŸ“¸ Comparing images:")
    print(f"   Image 1: {image1_path}")
    print(f"   Image 2: {image2_path}")
    
    result = comparator.compare_images(
        image1_path, 
        image2_path,
        use_visual_only=False,
        verbose=True
    )
    
    return result


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Compare 2 images using trained model")
    parser.add_argument('image1', type=str, help='Path to first image')
    parser.add_argument('image2', type=str, help='Path to second image')
    parser.add_argument('--backbone', type=str, default='./weights/backbone_full.pth',
                       help='Path to backbone checkpoint')
    parser.add_argument('--relation', type=str, default='./weights/relation_full.pth',
                       help='Path to relation net checkpoint')
    parser.add_argument('--concept-dir', type=str, default='./output_json/CUB200',
                       help='Path to concept embeddings directory')
    parser.add_argument('--visual-only', action='store_true',
                       help='Use only visual similarity, ignore concepts')
    
    args = parser.parse_args()
    
    demo_comparison(
        image1_path=args.image1,
        image2_path=args.image2,
        backbone_path=args.backbone,
        relation_path=args.relation,
        concept_dir=args.concept_dir
    )
