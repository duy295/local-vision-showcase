{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c0bfcbd",
   "metadata": {},
   "source": [
    "# Colab-ready notebook: Update Colab file by cell gid and test the updated `get_optimized_feature`\n",
    "\n",
    "This notebook provides utilities to programmatically load and edit Colab notebooks by gid (cell id), apply code style checks, run the modified notebook, and includes the corrected `get_optimized_feature` function (with spatial relations) plus smoke tests.\n",
    "\n",
    "Run on Colab: Runtime â†’ Change runtime type â†’ GPU (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef97d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this cell first)\n",
    "!pip install -q open_clip_torch Pillow requests nbformat nbclient papermill google-auth google-api-python-client gitpython black isort flake8 pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74936b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and lightweight checks\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "import nbformat\n",
    "from nbclient import NotebookClient\n",
    "import black\n",
    "import open_clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: load open-clip model (same as your project) \n",
    "\n",
    "def load_clip_model(model_name='ViT-B-32', pretrained='laion2b_s34b_b79k'):\n",
    "    \"\"\"Return: model (on device, eval()), preprocess, tokenizer\"\"\"\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained)\n",
    "    tokenizer = open_clip.get_tokenizer(model_name)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, preprocess, tokenizer\n",
    "\n",
    "# quick load (lazy: call when running tests to save time)\n",
    "model, preprocess, tokenizer = None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0695d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected get_optimized_feature (with spatial relations and optional relation fusion)\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_optimized_feature(json_data, model, tokenizer, device, preprocess=None,\n",
    "                          include_relation_in_fusion=False, relation_weight=0.1):\n",
    "    class_name = json_data['class_name']\n",
    "\n",
    "    # --- 1. GLOBAL FEATURE ---\n",
    "    global_text = f\"A photo of a {class_name}. {json_data.get('global_description','')}\"\n",
    "    with torch.no_grad():\n",
    "        global_tokens = tokenizer([global_text]).to(device)\n",
    "        global_emb = model.encode_text(global_tokens)\n",
    "        global_emb = global_emb / global_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # --- 2. LOCAL PARTS FEATURE ---\n",
    "    part_embs_list = []\n",
    "    if json_data.get('part_details'):\n",
    "        for part_name, description in json_data['part_details'].items():\n",
    "            part_text = f\"A close-up photo of the {part_name} of a {class_name}, described as {description}\"\n",
    "            tokens = tokenizer([part_text]).to(device)\n",
    "            emb = model.encode_text(tokens)\n",
    "            emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "            part_embs_list.append(emb)\n",
    "    if part_embs_list:\n",
    "        parts_tensor = torch.stack(part_embs_list).squeeze()\n",
    "        local_emb = torch.mean(parts_tensor, dim=0, keepdim=True)\n",
    "        local_emb = local_emb / local_emb.norm(dim=-1, keepdim=True)\n",
    "    else:\n",
    "        local_emb = torch.zeros_like(global_emb)\n",
    "\n",
    "    # --- 3. ATTRIBUTE FEATURE ---\n",
    "    attr_embs_list = []\n",
    "    if json_data.get('discriminative_attributes'):\n",
    "        for attr in json_data['discriminative_attributes']:\n",
    "            attr_text = f\"A photo of {class_name} with {attr}\"\n",
    "            tokens = tokenizer([attr_text]).to(device)\n",
    "            emb = model.encode_text(tokens)\n",
    "            emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "            attr_embs_list.append(emb)\n",
    "    if attr_embs_list:\n",
    "        attr_tensor = torch.stack(attr_embs_list).squeeze()\n",
    "        attr_emb = torch.mean(attr_tensor, dim=0, keepdim=True)\n",
    "        attr_emb = attr_emb / attr_emb.norm(dim=-1, keepdim=True)\n",
    "    else:\n",
    "        attr_emb = torch.zeros_like(global_emb)\n",
    "\n",
    "    # --- 3.5 IMAGE FEATURE (optional) ---\n",
    "    image_emb = torch.zeros_like(global_emb)\n",
    "    if json_data.get('image_path'):\n",
    "        if preprocess is None:\n",
    "            raise ValueError(\"preprocess is required to compute image embeddings. Pass preprocess from load_clip_model.\")\n",
    "        img = Image.open(json_data['image_path']).convert('RGB')\n",
    "        image_input = preprocess(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            image_emb = model.encode_image(image_input)\n",
    "            image_emb = image_emb / image_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # --- 3.6 RELATIONAL (spatial relations) ---\n",
    "    relation_embs_list = []\n",
    "    if json_data.get('spatial_relations'):\n",
    "        for part_name, description in json_data['spatial_relations'].items():\n",
    "            part_text = f\"A close-up photo of a {class_name}, with the part {description}\"\n",
    "            tokens = tokenizer([part_text]).to(device)\n",
    "            emb = model.encode_text(tokens)\n",
    "            emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "            relation_embs_list.append(emb)\n",
    "    if relation_embs_list:\n",
    "        parts_tensor = torch.stack(relation_embs_list).squeeze()\n",
    "        relation_embs = torch.mean(parts_tensor, dim=0, keepdim=True)\n",
    "        relation_embs = relation_embs / relation_embs.norm(dim=-1, keepdim=True)\n",
    "    else:\n",
    "        relation_embs = torch.zeros_like(global_emb)\n",
    "\n",
    "    # --- 4. FUSION ---\n",
    "    final_feature = 0.4 * global_emb + 0.25 * image_emb + 0.2 * local_emb + 0.15 * attr_emb\n",
    "    if include_relation_in_fusion:\n",
    "        final_feature = final_feature + relation_weight * relation_embs\n",
    "    final_feature = final_feature / final_feature.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return final_feature, global_emb, local_emb, image_emb, relation_embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoke tests: example JSONs and runs\n",
    "# Load model now (this may take a moment). Use GPU if available.\n",
    "model, preprocess, tokenizer = load_clip_model()\n",
    "\n",
    "json_no_image = {\n",
    "    \"class_name\": \"golden retriever\",\n",
    "    \"global_description\": \"a friendly medium-large dog with golden coat\",\n",
    "    \"part_details\": {\"face\": \"broad skull, kind eyes\", \"tail\": \"feathered\"},\n",
    "    \"discriminative_attributes\": [\"golden coat\", \"friendly expression\"],\n",
    "    \"spatial_relations\": {\"face\": \"front with eyes above nose\", \"tail\": \"rear, wagging\"}\n",
    "}\n",
    "\n",
    "final_feat, g, l, img, rel = get_optimized_feature(json_no_image, model, tokenizer, device, preprocess=None)\n",
    "print('No image run shapes -> final:', final_feat.shape, 'global:', g.shape, 'local:', l.shape, 'relation:', rel.shape)\n",
    "\n",
    "# With image: download sample and run\n",
    "img_url = 'https://images.unsplash.com/photo-1517841905240-472988babdf9?auto=format&fit=crop&w=400&q=80'\n",
    "resp = requests.get(img_url)\n",
    "open('/content/sample.jpg', 'wb').write(resp.content)\n",
    "json_no_image['image_path'] = '/content/sample.jpg'\n",
    "final_feat2, g2, l2, img2, rel2 = get_optimized_feature(json_no_image, model, tokenizer, device, preprocess=preprocess, include_relation_in_fusion=True, relation_weight=0.1)\n",
    "print('With image run shapes -> final:', final_feat2.shape, 'image:', img2.shape, 'relation:', rel2.shape)\n",
    "\n",
    "# Quick similarity checks\n",
    "cos = torch.nn.functional.cosine_similarity\n",
    "print('cos(final,global):', float(cos(final_feat2, g2).item()))\n",
    "print('cos(final,image):', float(cos(final_feat2, img2).item()))\n",
    "print('cos(final,relation):', float(cos(final_feat2, rel2).item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d378cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook-by-GID utilities (load/update/format/execute/diff/save)\n",
    "\n",
    "def load_notebook(path):\n",
    "    nb = nbformat.read(path, as_version=4)\n",
    "    return nb\n",
    "\n",
    "def map_gid_to_index(nb):\n",
    "    mapping = {}\n",
    "    for i, cell in enumerate(nb.cells):\n",
    "        gid = cell.get('id') or cell.get('metadata', {}).get('gid') or f'cell-{i}'\n",
    "        mapping[gid] = i\n",
    "    return mapping\n",
    "\n",
    "def update_cells_by_gid(nb, updates: dict):\n",
    "    \"\"\"updates: {gid: new_source_str}\"\"\"\n",
    "    mapping = map_gid_to_index(nb)\n",
    "    for gid, new_src in updates.items():\n",
    "        idx = mapping.get(gid)\n",
    "        if idx is None:\n",
    "            logging.warning('gid %s not found', gid)\n",
    "            continue\n",
    "        nb.cells[idx].source = new_src\n",
    "        nb.cells[idx].outputs = []\n",
    "    return nb\n",
    "\n",
    "def format_code_cells(nb):\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type == 'code':\n",
    "            try:\n",
    "                cell.source = black.format_str(cell.source, mode=black.Mode())\n",
    "            except Exception as e:\n",
    "                logging.warning('black failed: %s', e)\n",
    "    return nb\n",
    "\n",
    "def run_notebook(nb, timeout=300):\n",
    "    client = NotebookClient(nb, timeout=timeout, kernel_name='python3')\n",
    "    out = client.execute()\n",
    "    return nb\n",
    "\n",
    "def notebook_diff(nb_a, nb_b):\n",
    "    diffs = []\n",
    "    for i, (ca, cb) in enumerate(zip(nb_a.cells, nb_b.cells)):\n",
    "        if ca.source != cb.source:\n",
    "            diff = '\\n'.join(list(difflib.unified_diff(ca.source.splitlines(), cb.source.splitlines(), fromfile=f'a_cell_{i}', tofile=f'b_cell_{i}')))\n",
    "            diffs.append((i, diff))\n",
    "    return diffs\n",
    "\n",
    "# Example usage (local path):\n",
    "# nb = load_notebook('/content/my_colab.ipynb')\n",
    "# nb2 = update_cells_by_gid(nb, {'some-gid': \"print('new')\"})\n",
    "# nb2 = format_code_cells(nb2)\n",
    "# nb2 = run_notebook(nb2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d05a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final notes and short instructions\n",
    "print('\\nðŸ’¡ How to use this notebook:')\n",
    "print('1) Run the install cell once.\\n2) Run the model/test cells (may take a minute to download weights).\\n3) To apply edits to a Colab notebook file: load it with load_notebook(), map gids with map_gid_to_index(), apply update_cells_by_gid(), format_code_cells(), then run_notebook() to test outputs.')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
